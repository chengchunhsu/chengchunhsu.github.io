---
layout: homepage
---

## About Me
I am a Ph.D. student in Computer Science at the <a href="https://www.cs.utexas.edu/">University of Texas at Austin</a>. My research lies at the intersection of <i>robotics</i> and <i>computer vision</i>.

<!-- ## News -->

<h2 id="publications" style="margin: 2px 0px -15px;">Publications <temp style="font-size:15px;">[</temp><a href="https://scholar.google.com/citations?user=9rh0w2QAAAAJ" target="_blank" style="font-size:15px;">Google Scholar</a><temp style="font-size:15px;">]</temp></h2>

<div class="publications">
<ol class="bibliography">


<li>
<div class="pub-row">
  <div class="col-sm-3" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <!-- <img src="./assets/data/teaser_icra25_spot.jpg" class="teaser img-fluid z-depth-1"> -->
    <img src="./assets/data/teaser_icra25_spot.gif" class="teaser img-fluid z-depth-1">
  </div>
  <div id="hsu2022ditto" class="col-sm-9" style="position: relative;width: 100%;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://nvlabs.github.io/object_centric_diffusion/">SPOT: SE(3) Pose Trajectory Diffusion for Object-Centric Manipulation</a></div>
      <div class="author"><u><strong>Cheng-Chun Hsu</strong></u>, Bowen Wen, Jie Xu, Yashraj Narang, Xiaolong Wang, Yuke Zhu, Joydeep Biswas, Stan Birchfield</div>
      <div class="periodical"><em>International Conference on Robotics and Automation <strong>(ICRA)</strong>, 2025.</em>
      </div>
    <div class="links">
      <a href="https://nvlabs.github.io/object_centric_diffusion/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project Page</a>
      <a href="https://arxiv.org/abs/2411.00965" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Paper</a>
      <a href="https://github.com/NVlabs/object_centric_diffusion/tree/main" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
    <br>
    We enable robots to learn everyday tasks from human video demonstrations by usinbg object-centric representation. By predicting future object pose trajectories, SPOT achieves strong generalization capabilities with only eight human video demonstrations.
    <br>
  </div>
</div>
</li>

<br>

<li>
<div class="pub-row">
  <div class="col-sm-3" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <!-- <img src="./assets/data/teaser_icra24w_kinscene.jpeg" class="teaser img-fluid z-depth-1"> -->
    <img src="./assets/data/teaser_icra24w_kinscene.gif" class="teaser img-fluid z-depth-1">
  </div>
  <div id="hsu2022ditto" class="col-sm-9" style="position: relative;width: 100%;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://chengchunhsu.github.io/KinScene/">KinScene: Model-Based Mobile Manipulation of Articulated Scenes</a></div>
      <div class="author"><u><strong>Cheng-Chun Hsu</strong></u>, Ben Abbatematteo, Zhenyu Jiang, Yuke Zhu, Roberto Martín-Martín, Joydeep Biswas</div>
      <div class="periodical"><em></em>
      </div>
      <!-- <div class="periodical"><em>A Future Roadmap for Sensorimotor Skill Learning Workshop</em>
      </div> -->
      <div class="periodical"><em>Mobile Manipulation Workshop at ICRA, 2024.</em>
      </div>
      <strong><i style="color:#e74d3c">Spotlight Presentation</i></strong>
    <div class="links">
      <a href="https://chengchunhsu.github.io/KinScene/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project Page</a>
      <a href="https://arxiv.org/abs/2409.16473" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Paper</a>
    </div>
    <br>
    We enable mobile manipulators to perform long-horizon tasks by autonomously exploring and building scene-level articulation models of articulated objects. It maps the scene, infers object properties, and plans sequential interactions for accurate real-world manipulation.
    <br>
  </div>
</div>
</li>

<br>

<li>
<div class="pub-row">
  <div class="col-sm-3" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <!-- <img src="./assets/data/teaser_icra23_dittohouse.jpg" class="teaser img-fluid z-depth-1"> -->
    <img src="./assets/data/teaser_icra23_dittohouse.gif" class="teaser img-fluid z-depth-1">
  </div>
  <div id="hsu2022ditto" class="col-sm-9" style="position: relative;width: 100%;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://ut-austin-rpl.github.io/HouseDitto/">Ditto in the House: Building Articulated Models of Indoor Scenes through Interactive Perception</a></div>
      <div class="author"><u><strong>Cheng-Chun Hsu</strong></u>, Zhenyu Jiang, Yuke Zhu</div>
      <div class="periodical"><em>International Conference on Robotics and Automation <strong>(ICRA)</strong>, 2023.</em>
      </div>
    <div class="links">
      <a href="https://ut-austin-rpl.github.io/HouseDitto/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project Page</a>
      <a href="https://arxiv.org/abs/2302.01295" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Paper</a>
      <a href="https://github.com/UT-Austin-RPL/HouseDitto" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
    <br>
    We develop an interactive perception approach for robots to build indoor scene articulation models by efficiently discovering and characterizing articulated objects through coupled affordance prediction and articulation inference.
    <br>
  </div>
</div>
</li>

<br>

<li>
<div class="pub-row">
  <div class="col-sm-3" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <!-- <img src="./assets/data/teaser_cvpr22_ditto.jpg" class="teaser img-fluid z-depth-1"> -->
    <img src="./assets/data/teaser_cvpr22_ditto.gif" class="teaser img-fluid z-depth-1">
  </div>

  <!-- <div class="col-sm-3" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <video width=175px muted autoplay loop>
      <source src="./assets/data/teaser_cvpr22_ditto.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  </div> -->

  <!-- <div class="col-sm-3" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <div class="embed-responsive">
      <video width=175px outline:none muted autoplay loop>
        <source src="./assets/data/teaser_cvpr22_ditto.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>
  </div> -->

  <div id="jiang2022ditto" class="col-sm-9" style="position: relative;width: 100%;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://ut-austin-rpl.github.io/Ditto/">Ditto: Building Digital Twins of Articulated Objects from Interaction</a></div>
      <div class="author">Zhenyu Jiang, <u><strong>Cheng-Chun Hsu</strong></u>, Yuke Zhu</div>
      <div class="periodical"><em>Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2022.</em>
      </div>
      <strong><i style="color:#e74d3c">Oral Presentation</i></strong>
    <div class="links">
      <a href="https://ut-austin-rpl.github.io/Ditto/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project Page</a>
      <a href="https://arxiv.org/abs/2202.08227" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Paper</a>
      <a href="https://github.com/UT-Austin-RPL/Ditto" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
    <br>
    We develop an approach that builds digital twins of articulated objects by learning their articulation models and 3D geometry from visual observations before and after interaction.
    <br>
  </div>
</div>
</li>

<br>

<li>
<div class="pub-row">
  <!-- <div class="col-sm-3 center-block" style="position: relative;padding-right: 15px;padding-left: 15px;"> -->
  <div class="col-sm-3" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="./assets/data/teaser_eccv20_epm.jpg" class="teaser img-fluid z-depth-1">
  </div>
  <!-- <td style="padding:2.5%;width:30%;vertical-align:middle;min-width:120px">
  <img src="./assets/data/teaser_eccv20_epm.jpg" alt style="width:auto; height:auto; max-width:30%;">
  <!-- <abbr class="badge">ECCV</abbr> -->
  <!-- </td> -->
  <div id="hsu2020every" class="col-sm-9" style="position: relative;width: 100%;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://chengchunhsu.github.io/EveryPixelMatters/">Every Pixel Matters: Center-aware Feature Alignment for Domain Adaptive Object Detector</a></div>
      <div class="author"><u><strong>Cheng-Chun Hsu</strong></u>, Yi-Hsuan Tsai, Yen-Yu Lin, and Ming-Hsuan Yang</div>
      <div class="periodical"><em>European Conference on Computer Vision <strong>(ECCV)</strong>, 2020.</em>
      </div>
    <div class="links">
      <a href="https://chengchunhsu.github.io/EveryPixelMatters/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project Page</a>
      <a href="https://arxiv.org/abs/2008.08574" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Paper</a>
      <a href="https://github.com/chengchunhsu/EveryPixelMatters" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
    <br>
    We propose a domain adaptation framework for object detection that uses pixel-wise objectness and centerness to align features, focusing on foreground pixels for better cross-domain adaptation.
    <br>
  </div>
</div>
</li>

<br>

<li>
<div class="pub-row">
  <div class="col-sm-3" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="./assets/data/teaser_neurips19_bbtp.jpg" class="teaser img-fluid z-depth-1">
  </div>
  <div id="hsu2019weakly" class="col-sm-9" style="position: relative;width: 100%;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://papers.nips.cc/paper/2019/file/e6e713296627dff6475085cc6a224464-Paper.pdf">Weakly Supervised Instance Segmentation using the Bounding Box Tightness Prior</a></div>
      <div class="author"><u><strong>Cheng-Chun Hsu</strong></u>*, Kuang-Jui Hsu*, Chung-Chi Tsai, Yen-Yu Lin, and Yung-Yu Chuang</div>
      <div class="periodical"><em>Neural Information Processing Systems <strong>(NeurIPS)</strong>, 2019.</em>
      </div>
    <div class="links">
      <a href="https://papers.nips.cc/paper/2019/file/e6e713296627dff6475085cc6a224464-Paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Paper</a>
      <a href="https://github.com/chengchunhsu/WSIS_BBTP/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
    </div>
    <br>
    We propose a weakly supervised instance segmentation method that leverages Multiple Instance Learning (MIL) to address ambiguous foreground separation from bounding box annotations.
    <br>
  </div>
</div>
</li>

<br>

<li>
<div class="pub-row">
  <div class="col-sm-3" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="./assets/data/teaser_mm18_fashion.jpg" class="teaser img-fluid z-depth-1">
  </div>
  <div id="hidayati2018fashion" class="col-sm-9" style="position: relative;width: 100%;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="./assets/data/pub_mm18_fashion.pdf">What Dress Fits Me Best? Fashion Recommendation on the Clothing Style for Personal Body Shape</a></div>
      <div class="author">Shintami Chusnul Hidayati, <u><strong>Cheng-Chun Hsu</strong></u>, Yu-Ting Chang, Kai-Lung Hua, Jianlong Fu, and Wen-Huang Cheng</div>
      <div class="periodical"><em>ACM International Conference on Multimedia <strong>(MM)</strong>, 2018.</em>
      </div>
      <strong><i style="color:#e74d3c">Oral Presentation</i></strong>
    <div class="links">
      <a href="./assets/data/pub_mm18_fashion.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Paper</a>
    </div>
    <br>
    We propose to learn clothing style and body shape compatibility from social big data, offering personalized outfit recommendations by factoring in a user's body shape.
    <br>
  </div>
</div>
</li>

</ol>
</div>



<h2 id="publications" style="margin: 2px 0px -15px;">Technical Reports</h2>

<div class="publications">
<ol class="bibliography">

<li>
<div class="pub-row">
  <div class="col-sm-3" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="./assets/data/teaser_techreport19_ccgr.jpg" class="teaser img-fluid z-depth-1">
  </div>
  <div id="hsu2019center" class="col-sm-9" style="position: relative;width: 100%;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="./assets/data/pub_techreport19_ccgr.pdf">Center-context-gap Refinement for Weakly Supervised Instance Segmentation</a></div>
      <div class="author"><u><strong>Cheng-Chun Hsu</strong></u>*, Kuang-Jui Hsu*, Chiachen Ho, Yen-Yu Lin, and Yung-Yu Chuang</div>
      <div class="periodical"><em>Technical report, 2019.</em>
      </div>
    <div class="links">
      <a href="./assets/data/pub_techreport19_ccgr.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Paper</a>
    </div>
    <br>
    We propose a weakly supervised instance segmentation method using image-level labels, leveraging MIL, semantic segmentation, and a novel refinement module.
    <br>
  </div>
</div>
</li>

</ol>
</div>



<!-- ## Services
Conference Reviewers: NeurIPS, ICLR, ECCV, WACV, AAAI, IJCAI -->